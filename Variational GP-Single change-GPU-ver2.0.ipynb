{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(i, N, tstart):\n",
    "    # display time elapsed and time remaining within for loop\n",
    "    #\n",
    "    # [T, Trem] = display_progress(i, N, tstart)\n",
    "    #\n",
    "    # INPUTS\n",
    "    # i:    current iteration\n",
    "    # N:    total number of iterations\n",
    "    # tstart: time at start (if empty, then function calls 'tic' with no argument)\n",
    "    # display (default = true): display time elapsed etc.\n",
    "    #\n",
    "    # OUTPUTS\n",
    "    # T:    time elapsed\n",
    "    # Trem: time remaining\n",
    "\n",
    "    T = time.time() - tstart\n",
    "    Trem = T*(N-(i+1) + 1)/(i+1)\n",
    "\n",
    "    # DEFAULT('display', true);\n",
    "    print(f'percent complete {(i)/N*100:.0f}%')\n",
    "    if i>0:\n",
    "        print(f'{T/60:.1f} mins passed, {Trem/60:.1f} mins remaining')\n",
    "    return T, Trem\n",
    "\n",
    "#Note that in python, we use time.time() to get the current time instead of tic and toc. Also, the nargin parameter is not \n",
    "#used in python, so it's removed from the code. Additionally, the fprintf is replaced with print(f'{}') in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estep(m, f, r, A, K, a):\n",
    "# perform one update of m and V\n",
    "\n",
    "    g = A*a @ (r.T - f)   #shape(ntilde,1)\n",
    "    G = A**2 *a @ (a.T* f)    #shape(ntilde,ntilde)\n",
    "    V = np.linalg.solve(np.eye(K.shape[0])+K @ G, K)\n",
    "    #V = np.linalg.solve(V,( (1-Alpha)*K+Alpha*V+Alpha*K@G@V ) )@K\n",
    "    V = (V + V.T) / 2 + 1e-15 * np.eye(K.shape[0]) # make sure positive definite!\n",
    "    m = V @ (G @ m + g)  #shape(250,1)\n",
    "    #m = m - Alpha*((eye(ntilde)+K*G)\\(m-K*g))\n",
    "    \n",
    "    return m, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localsmoothkern(theta, n):  #function to calculate C given a set of hyperparameters\n",
    "    # INPUT:\n",
    "    # theta, 4 hyperparameters determinine shape and position of RF\n",
    "    # idx selects dimension where C>>0 \n",
    "    # dC derivative of C with respect to each hyperparameter\n",
    "\n",
    "    # hyperparameters\n",
    "    rho = np.exp(theta[0])    # how smooth is RF?\n",
    "    x0  = theta[1]         # spatial location, xcoord\n",
    "    y0  = theta[2]         # spatial location, ycoord\n",
    "    beta = np.exp(theta[3])   # how wide is RF?\n",
    "\n",
    "    # spatial localised prior\n",
    "    ycord, xcord = np.meshgrid(np.linspace(-1, 1, int(np.sqrt(n))), np.linspace(-1, 1, int(np.sqrt(n)))) #a grid of 108x108 points between -1 and 1\n",
    "    xcord = xcord.flatten()\n",
    "    ycord = ycord.flatten()\n",
    "    logf = -0.5*beta*((xcord- x0)**2+(ycord- y0)**2)\n",
    "    f = np.exp(logf)    #aplha_local in the paper\n",
    "    \n",
    "    # select only stimulus dimensions near RF centre\n",
    "    idx = f>0.001\n",
    "    f = f[idx]\n",
    "    logf = logf[idx]\n",
    "    xcord = xcord[idx]\n",
    "    ycord = ycord[idx]\n",
    "    n = len(f)\n",
    "\n",
    "    # smooth prior\n",
    "    logK0 = -0.5*rho*((xcord - xcord[:, np.newaxis])**2+(ycord - ycord[:, np.newaxis])**2)\n",
    "    K0 = np.exp(logK0)   #cij smooth in the paper\n",
    "\n",
    "    # multiply smoothness prior and spatial localised prior\n",
    "    C = f[:, np.newaxis]*K0*f[np.newaxis, :]\n",
    "    #print(f.shape, K0.shape, C.shape)\n",
    "\n",
    "    # derivative with repect to different hyperparameters \n",
    "    dC = np.zeros((C.shape[0], C.shape[1], len(theta)))\n",
    "# dC : array-like, optional\n",
    "#         [nx, nx, ntheta], derivative of C with respect to hyperparameters\n",
    "    dC[:, :, 0] = logK0*C\n",
    "    dC[:, :, 1] = beta*C*(xcord[:, np.newaxis]+xcord[np.newaxis, :]- 2*x0)\n",
    "    dC[:, :, 2] = beta*C*(ycord[:, np.newaxis]+ycord[np.newaxis, :]- 2*y0)\n",
    "    dC[:, :, 3] = C*(logf[:, np.newaxis] + logf[np.newaxis, :])\n",
    "\n",
    "    # make sure that K is positive definite\n",
    "    C = (C+C.T)/2 #+ 1e-10*np.eye(n)\n",
    "    #print('n:{}'.format(n))\n",
    "    #print(xcord.shape, ycord.shape, f.shape, logf.shape)\n",
    "    return C, idx, dC          #the matrix will be nxn and the value n depends on the threshold on f. n=500 for thresh=0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateA(A, lambda0, mstar0, Vstar0, r, nit=1000, Alpha=0.25):\n",
    "    psi = [lambda0, A]\n",
    "    count = 0 \n",
    "    flag = True\n",
    "    L = np.zeros((int(nit), 1))\n",
    "    \n",
    "    while flag:\n",
    "        count  = count+1\n",
    "        f = np.exp(psi[1]*mstar0 + 0.5*Vstar0*psi[1]**2+psi[0])  #shape (3190,1)\n",
    "        L[count-1] = np.sum(f) - r@(mstar0*psi[1]+psi[0])\n",
    "        dlambda_A = mstar0 + Vstar0*psi[1]             #shape (3190,1)\n",
    "        g = np.array([np.sum(f-r.T),  (f.T@dlambda_A - r@mstar0)[0,0] ])\n",
    "        H = np.array([[np.sum(f), (f.T@dlambda_A)[0,0]], [(f.T@dlambda_A)[0,0], (f.T@Vstar0 + (dlambda_A*f).T@dlambda_A)[0,0]]])\n",
    "        \n",
    "        psi = psi - Alpha*np.linalg.solve(H,g)\n",
    "\n",
    "        if np.sum(abs(g))<1e-6 or count>nit:\n",
    "            flag = False\n",
    "        \n",
    "    \n",
    "    A = psi[1] \n",
    "    lambda0 = psi[0]\n",
    "    L = L[0:count]\n",
    "    return A, lambda0, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acoskern(theta, x1, x2, C=None, dC=None, diag=False):\n",
    "    \"\"\"\n",
    "    arc cosine covariance function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array-like\n",
    "        hyperparameters, first 2 are for acos kernel, the rest are for C\n",
    "    x1 : array-like\n",
    "        [nx, n1] matrix, first input   #(500,ntilde or r.shape)\n",
    "    x2 : array-like\n",
    "        [nx, n2] matrix, second input  #(500, ntilde or r.shape)\n",
    "    C : array-like, optional\n",
    "        [nx, nx] matrix, smooth local covariance matrix\n",
    "    dC : array-like, optional\n",
    "        [nx, nx, ntheta], derivative of C with respect to hyperparameters\n",
    "    diag : bool, optional\n",
    "        if diag ==1, then just return diagonal elements of covariance matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : array-like\n",
    "        [n1, n2] kernel\n",
    "    dK : array-like\n",
    "        [n1, n2, ntheta], derivative of kernel with respect to theta\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from numpy import pi\n",
    "\n",
    "    if diag is None:\n",
    "        diag = False\n",
    "\n",
    "    if C is None:\n",
    "        C = np.eye(x1.shape[0])\n",
    "\n",
    "    n1 = x1.shape[1]\n",
    "\n",
    "    sigmab = np.exp(theta[0])\n",
    "\n",
    "    if not diag:\n",
    "        n2 = x2.shape[1]\n",
    "        \n",
    "        X1 = np.sqrt(np.sum(x1*(C @ x1), axis=0) + sigmab ** 2) # np.sum(x1*(C@x1), axis=0) is the same as x1.T @ C @ x1 #shape(n1)\n",
    "        X2 = np.sqrt(np.sum(x2*(C @ x2), axis=0) + sigmab ** 2) # shape(n2,)\n",
    "        #print((C @ x2)[19,0])\n",
    "        X1X2 = np.outer(X1,X2)               #shape(n1,n2)\n",
    "        x1x2 = x1.T @ C @ x2 + sigmab ** 2   \n",
    "        \n",
    "        arg = np.clip(x1x2 / (X1X2 + 1e-9), -1, 1)\n",
    "\n",
    "        theta = np.arccos(arg)\n",
    "\n",
    "        J = (np.sqrt(1 - arg ** 2) + np.pi * arg - theta * arg) / np.pi   #shape(n1,n2)\n",
    "\n",
    "        K = X1X2 * J       #shape(n1, n2)\n",
    "        \n",
    "        #print('X1:{},X2:{}, J:{}, X1X2:{}, K:{} '.format(X1.shape, X2.shape, J.shape, X1X2.shape, K.shape))\n",
    "        if dC is not None:\n",
    "            dK = np.zeros((n1, n2, dC.shape[2] + 1))\n",
    "#     dK : array-like\n",
    "#         [n1, n2, ntheta], derivative of kernel with respect to theta\n",
    "\n",
    "            dX1X2 = sigmab ** 2 * (X2 / X1[:, np.newaxis] + X1[:, np.newaxis] / X2) #shape same as X1X2\n",
    "\n",
    "            darg = (2 * sigmab ** 2 - arg * dX1X2) / X1X2    #shape same as arg\n",
    "\n",
    "            dJ = -(theta - np.pi) * darg / np.pi           #shapesame as J\n",
    "\n",
    "            dK[:, :, 0] =  (X1X2 * dJ + dX1X2 * J)      #shape same as K\n",
    "                                   \n",
    "            for j in range(1, dC.shape[2] + 1):\n",
    "\n",
    "                dX1 = 0.5*np.sum(x1*np.dot(dC[:, :, j-1], x1), axis=0)/X1  #shape(n1,)\n",
    "                dX2 = 0.5*np.sum(x2*np.dot(dC[:, :, j-1], x2), axis=0)/X2  #shape(n2,)\n",
    "                \n",
    "                dX1X2 = dX1[:, np.newaxis]*X2 + X1[:, np.newaxis]*dX2\n",
    "\n",
    "                darg = (np.dot(x1.T, np.dot(dC[:, :, j-1], x2)) - arg*dX1X2)/X1X2\n",
    "\n",
    "                dJ =  -(theta-np.pi)*darg/np.pi\n",
    "\n",
    "                dK[:, :, j] = X1X2*dJ + dX1X2*J\n",
    "            \n",
    "            #print('dX1X2:{}, darg:{}, dJ:{}, dK[:,:,1]:{}'.format(dX1X2.shape,darg.shape, dJ.shape, (A * (X1X2 * dJ + dX1X2 * J)).shape))\n",
    "            # make sure that K is positive definite\n",
    "        if n1==n2:\n",
    "            K = (K+K.T)/2 + 1e-15*np.eye(n1)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # return just diagonal of covariance\n",
    "        K = np.sum(x1*np.dot(C, x1), axis=0)[:, np.newaxis]+sigmab**2\n",
    "\n",
    "\n",
    "        # derivative of covariance\n",
    "        if dC is not None:\n",
    "            dK = np.zeros((n1,1, dC.shape[2] + 1))\n",
    "\n",
    "            dK[:,:, 0] = 2*sigmab**2*np.ones((n1, 1))\n",
    "\n",
    "            for j in range(1, dC.shape[2] + 1):\n",
    "                dK[:,:, j] = np.sum(x1 * np.dot(dC[:, :, j-1], x1), axis=0)[:, np.newaxis]\n",
    "\n",
    "            K += 1e-15 * np.eye(n1, 1)\n",
    "\n",
    "    if dC is not None: return K, dK    #shape(n1,n2), shape(n1,n2,6) \n",
    "    else: return K    #shape (n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute lsta and connected functions\n",
    "def compute_f(m, V, theta, A, lambda0, xtilde, x, kernfun=None):\n",
    "    if kernfun is None:\n",
    "        kernfun = acoskern\n",
    "\n",
    "    nx = x.shape[0]\n",
    "\n",
    "    # get C and index\n",
    "    C, idx,_ = localsmoothkern(theta[1:], nx)\n",
    "    #print(C[31,23])\n",
    "    # compute covariance matrices\n",
    "    K = kernfun(theta, xtilde[idx,:], xtilde[idx, :], C)     #shape(ntilde, ntilde)\n",
    "    k = kernfun(theta, xtilde[idx, :], x[idx, :], C)         #shape(ntilde,1)\n",
    "    kstar = kernfun(theta, x[idx, :], x[idx, :], C, None, True)  #shape(1,1)\n",
    "    #print(K[2,4], k[4],kstar)\n",
    "    #print(xtilde[5,33])\n",
    "    \n",
    "    # compute a\n",
    "    a = np.linalg.solve(K, k)    #shape (ntilde,1)\n",
    "    #a = K/k\n",
    "#     print(a[4])\n",
    "\n",
    "    # compute mstar\n",
    "    mstar = A*a.T@m + lambda0\n",
    "#     print(mstar)\n",
    "    # compute Vstar\n",
    "    Vstar = A**2*(kstar + np.sum(-k*a + a*V@a, axis=0)[:, np.newaxis])\n",
    "#     print(Vstar)\n",
    "    # compute fmean and sigma2_f\n",
    "    fmean = np.exp(mstar + 0.5*Vstar).T             #shape(1,1)\n",
    "    sigma2_f = (np.exp(Vstar**2) - 1)*fmean**2      #shape(1,1) \n",
    "    #print(fmean, sigma2_f)\n",
    "    return fmean, sigma2_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varGP(x, r, Nestep=50, Nmstep=20, Display=2, MaxIter=200, ntilde=250, *kwargs):\n",
    "    # learn gaussian process model (GP) of neural responses\n",
    "    # \n",
    "    # INPUTS\n",
    "    # x = [nx, n], stimulus\n",
    "    # r = [1, n],  spike counts\n",
    "    \n",
    "    # OPTIONAL INPUTS\n",
    "    # Nestep: number of  of iterations in E-step (updating m, and V)\n",
    "    # Nmstep: number of steps in M-step (updating theta )\n",
    "    # Display: 0/1/2, display nothing/progress/plots\n",
    "    # ntilde: number of inducing data points (how accurate is approximation)\n",
    "    # theta: initial hyperparameters\n",
    "    # lb, ub: lower and upper bound for theta\n",
    "    # kernfun: kernel function (acoskern is default)\n",
    "    # m, V: initial mean and variance of variational distribution, q\n",
    "    # \n",
    "    # OUTPUTS\n",
    "    # theta, hyperparameters\n",
    "    # L, loss function during learning\n",
    "    # m, V: mean and variance of variational distribution q(lambda) = N(m, V)\n",
    "    # xtilde: set of 'inducing' datapoints\n",
    "    \n",
    "    # optional arguments (described above)\n",
    "    opts = { \n",
    "            'theta': [ 0, 5, 0, 0, 5.5], # sigma_b, log(rho), eps_0_x, eps_0_y, log(beta)\n",
    "            'A':1e-4,\n",
    "            'lambda0':-1,\n",
    "            'lb': [ -float(\"inf\"), -float(\"inf\"), -1, -1, 4 ], \n",
    "            'ub': [float(\"inf\"), float(\"inf\"), 1, 1, float(\"inf\")], \n",
    "            'kernfun': acoskern, \n",
    "            'm': [], \n",
    "            'V': [], \n",
    "            'xtilde': []}\n",
    "    opts.update(kwargs)\n",
    "\n",
    "    #r = r.flatten()          # reformat spike count  \n",
    "\n",
    "    nx, n = x.shape     # number of data points\n",
    "    \n",
    "    # inducing points (chosen at random from data points)\n",
    "    if not opts['xtilde']:\n",
    "        ntilde = min(n, ntilde)\n",
    "        xtilde = x[:, np.random.permutation(n)[:ntilde]]+1e-6*np.random.randn(nx, ntilde) #shape(11664, ntilde)\n",
    "    else:\n",
    "        xtilde = opts['xtilde']\n",
    "        ntilde = xtilde.shape[1]\n",
    "\n",
    "    # initialize mean and covariance of q(xtilde), to prior\n",
    "    if not opts['m']:\n",
    "        m = np.zeros((ntilde, 1))    #shape (ntilde, 1)\n",
    "    else:\n",
    "        m = opts['m']\n",
    "    \n",
    "    if not opts['V']:\n",
    "        C, idx, dC = localsmoothkern(opts['theta'][1:], nx) #the C is calculated only for pixels around epsilon_0\n",
    "        V = opts['kernfun'](opts['theta'], xtilde[idx, :], xtilde[idx, :], C=C)  #(ntilde, ntilde)\n",
    "    else:\n",
    "        V = opts['V']\n",
    "    #print(log_det(C))  \n",
    "    # track loss and parameters\n",
    "    L = np.zeros((MaxIter, 1))         # log marginal\n",
    "    theta_track = np.zeros((5, MaxIter)) # track hyperparamers\n",
    "    A_track = np.zeros((MaxIter, 1))       # track hyperparamers\n",
    "    lambda0_track = np.zeros((MaxIter, 1))   # track hyperparamers\n",
    "    \n",
    "    # options for optimization in M-step\n",
    "    options = { 'maxiter': Nmstep}    \n",
    "\n",
    "    theta=opts['theta']\n",
    "    A=opts['A']\n",
    "    lambda0=opts['lambda0']\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for Iteration in tqdm(range(MaxIter)):\n",
    "#         if Display>0:\n",
    "#             display_progress(Iteration, MaxIter, t0)\n",
    "            \n",
    "#%%%%%%%%%%%%%% E- step  (update q = N(m, V))$$$$$$$$$$$$$\n",
    "\n",
    "        # local smooth covariance function\n",
    "        C, idx, dC = localsmoothkern(theta[1:], nx)\n",
    "\n",
    "        # pre-compute kernel\n",
    "        K = opts['kernfun'](theta, xtilde[idx, :], xtilde[idx, :], C=C) #shape (ntilde,ntilde) #K_ij in the notes\n",
    "        k = opts['kernfun'](theta, xtilde[idx, :], x[idx, :], C=C)     #shape(ntilde,3190)     #k_i  in the notes\n",
    "        kstar = opts['kernfun'](theta, x[idx, :], [], C=C, diag=True)   #shape(3190,1)          #k_ii in the notes\n",
    "        a = np.linalg.solve(K, k)                                       #shape(ntilde,3190)\n",
    "        #print(K.shape,k.shape,kstar.shape,a.shape)\n",
    "\n",
    "        for i in range(Nestep):\n",
    "            mstar0 = a.T@m     #mean of lambda(x) #shape (3190,1)\n",
    "            Vstar0 = kstar + np.sum(-k*a + a*(V@a), 0)[:, np.newaxis]  # variance of lambda(x) #shape (3190,1)\n",
    "            A, lambda0, _ = updateA(A, lambda0, mstar0, Vstar0, r, 1e4)\n",
    "        \n",
    "            f = np.exp(A*mstar0+ 0.5*Vstar0*A**2 + lambda0); # <f(x)> mean firing rate\n",
    "            \n",
    "            m, V = Estep(m, f, r, A, K, a)\n",
    "            \n",
    "            #print('{}:{}:  {},{}'.format(Iteration,i,m.shape,V.shape))\n",
    "        #print(logmarginal(m, V, theta, xtilde, x, r, opts['kernfun']))\n",
    "\n",
    "\n",
    "#In python, you can use the scipy.optimize library to perform gradient descent. Specifically, you can use the minimize \n",
    "#function with the \"BFGS\" algorithm to optimize the log marginal function. The equivalent code would be:\n",
    "\n",
    "#%%%%%%%%%%%%%% M- step (update hyperparameters, theta) $$$$$$$$$$$$$\n",
    "        bnds =((opts['lb'][0], opts['ub'][0]), (opts['lb'][1], opts['ub'][1]),(opts['lb'][2], opts['ub'][2]),(opts['lb'][3], opts['ub'][3]),(opts['lb'][4], opts['ub'][4]))\n",
    "        # gradient descent \n",
    "        result = minimize(lambda theta: logmarginal(m, V, theta, A, lambda0, xtilde, x, r, acoskern_1),\n",
    "                          theta,args=(), method='L-BFGS-B', jac=True, bounds=bnds, options=options)\n",
    "        \n",
    "        theta = result.x\n",
    "        L[Iteration] = result.fun\n",
    "        theta_track[:, Iteration] = theta\n",
    "        A_track[Iteration] = A\n",
    "        lambda0_track[Iteration] = lambda0\n",
    "#Note: I have replaced minConf_TMP with minimize function from scipy\n",
    "\n",
    "    if Display > 1:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(-L[1:])\n",
    "        plt.title('loss function')\n",
    "        plt.subplot(2, 2, 2)\n",
    "        for hyp in range(5):\n",
    "            plt.plot(theta_track[hyp, :]- theta_track[hyp, 0])\n",
    "        plt.title('hyperparameters')\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(A_track[1:])\n",
    "        plt.title('A')\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(lambda0_track[1:])\n",
    "        plt.title('lambda0')\n",
    "        plt.savefig(r'/media/samuele/Samuele_02/GP from MAtlab to python/Python/trial_3.png')\n",
    "        plt.close()\n",
    "        print('Done!')\n",
    "            \n",
    "    return theta_track, A_track, lambda0_track, m, V, xtilde, L "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#varGP and connected functions\n",
    "def log_det(M):\n",
    "    M.to(torch.float64)\n",
    "    try:\n",
    "        L=torch.linalg.cholesky(M, upper=True)\n",
    "        return 2*torch.sum(torch.log(torch.diag(L)))\n",
    "    except:\n",
    "        L = torch.linalg.eig(M)\n",
    "        L = L[L>1e-6]\n",
    "        return torch.sum(torch.log(L)) \n",
    "\n",
    "def logmarginal(m, V, theta, A, lambda0, xtilde, x, r, kernfun):\n",
    "    # compute negative log-evidence\n",
    "    nx = x.shape[0]\n",
    "\n",
    "    # precompute kernel terms\n",
    "    C, idx, dC = localsmoothkern(theta[1:], nx)\n",
    "    \n",
    "#     Sigma=torch.from_numpy(Sigma).to(device)\n",
    "#     dSigma=torch.from_numpy(dSigma).to(device)\n",
    "    C=torch.from_numpy(C).to(device)\n",
    "    dC=torch.from_numpy(dC).to(device)\n",
    "    m=torch.from_numpy(m).to(torch.float64).to(device)\n",
    "    V=torch.from_numpy(V).to(torch.float64).to(device)\n",
    "    A=torch.from_numpy(np.array(A)).to(device)\n",
    "    lambda0=torch.from_numpy(np.array(lambda0)).to(device)\n",
    "    r=torch.from_numpy(r.astype(np.float64)).to(device)\n",
    "    theta=torch.from_numpy(theta).to(device)\n",
    "    xtilde=torch.from_numpy(xtilde).to(device)\n",
    "    x=torch.from_numpy(x).to(device)\n",
    "    \n",
    "    Sigma, dSigma = kernfun(theta, xtilde[idx, :], xtilde[idx, :], C, dC)\n",
    "    ki, dki = kernfun(theta, xtilde[idx, :], x[idx, :], C, dC)\n",
    "    kstar, dkstar = kernfun(theta, x[idx, :], [], C, dC, True)\n",
    "#     print(dSigma)\n",
    "#     ki=torch.from_numpy(ki).to(device)\n",
    "#     kstar=torch.from_numpy(kstar).to(device)\n",
    "#     dki=torch.from_numpy(dki).to(device)\n",
    "#     dkstar=torch.from_numpy(dkstar).to(device)\n",
    "    \n",
    "    # compute log-likelihood \n",
    "    lkhd, dlkhd = lfun(m, V, A, lambda0, r, Sigma, ki, kstar, dSigma, dki, dkstar)\n",
    "    \n",
    "    # compute KL divergence\n",
    "    Dkl, dDkl = KLdiv(m, V, Sigma, dSigma)\n",
    "    dL = dDkl - dlkhd\n",
    "\n",
    "    # compute total loss\n",
    "    L = Dkl - lkhd        #shape (1,1)\n",
    "    #print(Dkl, lkhd)\n",
    "    return L[0,0].detach().cpu().numpy().astype(np.float64), dL.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "def lfun(m, V, A, lambda0, r, Sigma, ki, kstar, dSigma, dki, dkstar):\n",
    "    # log-likelihood\n",
    "\n",
    "    # compute mean and variance\n",
    "    a = torch.linalg.inv(Sigma)@ ki  #shape(ntilde, 3190)\n",
    "\n",
    "    mstar = A*a.T @ m + lambda0           #shape(3190,1)\n",
    "    Vstar = A**2*(kstar + torch.sum(-ki * a + a * (V @ a), axis=0)[:, None])  #shape(3190,1)\n",
    "\n",
    "    # mean firing rate\n",
    "    f = torch.exp(mstar + 0.5 * Vstar)\n",
    "\n",
    "    # log likelihood\n",
    "    Lkhd = r @ mstar - torch.sum(f)\n",
    "\n",
    "    # derivative of log likelihood with respect to theta\n",
    "    nparams = dSigma.shape[2]\n",
    "    dLtheta = torch.zeros(nparams)\n",
    "    for i in range(nparams):\n",
    "        da = torch.linalg.inv(Sigma)@ (dki[:, :, i] - dSigma[:, :, i] @ a)  #shape same as a\n",
    "        \n",
    "        dVstar = A**2*(dkstar[:,:, i] + torch.sum(-dki[:, :, i] * a - ki * da + 2 * da * (V @ a), axis=0)[:, None])  #shape same as Vstar\n",
    "\n",
    "        dmstar = A*da.T @ m    #shape same as mstar\n",
    "\n",
    "        dLtheta[i] = -0.5 * dVstar.T @ f + dmstar.T @ (r.T - f)\n",
    "\n",
    "    return Lkhd, dLtheta\n",
    "\n",
    "\n",
    "def KLdiv(m, V, Sigma, dSigma):\n",
    "    # KL divergence between prior and posterior\n",
    "#     C = np.linalg.solve(V,Sigma)        #shape(ntilde, ntilde)\n",
    "    C = V@torch.linalg.inv(Sigma)        #shape(ntilde, ntilde)\n",
    "    b = torch.linalg.inv(Sigma)@m        #shape(ntilde, 1)\n",
    "#     print('C:{}, b:{}'.format(C[1,4],b[6]))\n",
    "    \n",
    "    #print(np.log(np.linalg.det(V)), np.log(np.linalg.det(Sigma)), np.trace(C), m.T.dot(b))\n",
    "    #KL divergence\n",
    "    Dkl = 0.5*log_det(Sigma) -0.5*log_det(V) + 0.5*torch.trace(C) + 0.5*m.T@b\n",
    "\n",
    "    # derivative with respect to theta\n",
    "    nparams = dSigma.shape[2]\n",
    "    dDtheta = torch.zeros(nparams)\n",
    "    for i in range(nparams):\n",
    "        B = dSigma[:, :, i]@torch.linalg.inv(Sigma)\n",
    "\n",
    "        dDtheta[i] = 0.5*torch.trace(B) - 0.5*torch.trace(C@B) - 0.5*b.T@(B@m)\n",
    "\n",
    "    #print(Dkl.shape, dDtheta.shape)\n",
    "    return Dkl, dDtheta    #shape (1,1), shape (7,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acoskern_1(theta, x1, x2, C=None, dC=None, diag=False):\n",
    "    \"\"\"\n",
    "    arc cosine covariance function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : array-like\n",
    "        hyperparameters, first 2 are for acos kernel, the rest are for C\n",
    "    x1 : array-like\n",
    "        [nx, n1] matrix, first input   #(500,ntilde or r.shape)\n",
    "    x2 : array-like\n",
    "        [nx, n2] matrix, second input  #(500, ntilde or r.shape)\n",
    "    C : array-like, optional\n",
    "        [nx, nx] matrix, smooth local covariance matrix\n",
    "    dC : array-like, optional\n",
    "        [nx, nx, ntheta], derivative of C with respect to hyperparameters\n",
    "    diag : bool, optional\n",
    "        if diag ==1, then just return diagonal elements of covariance matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : array-like\n",
    "        [n1, n2] kernel\n",
    "    dK : array-like\n",
    "        [n1, n2, ntheta], derivative of kernel with respect to theta\n",
    "    \"\"\"\n",
    "\n",
    "    if diag is None:\n",
    "        diag = False\n",
    "\n",
    "    if C is None:\n",
    "        C = torch.eye(x1.shape[0]).to(device)\n",
    "\n",
    "    n1 = x1.shape[1]\n",
    "\n",
    "    sigmab = torch.exp(theta[0])\n",
    "\n",
    "    if not diag:\n",
    "        n2 = x2.shape[1]\n",
    "        \n",
    "        X1 = torch.sqrt(torch.sum(x1*(C @ x1), axis=0) + sigmab ** 2) # np.sum(x1*(C@x1), axis=0) is the same as x1.T @ C @ x1 #shape(n1)\n",
    "        X2 = torch.sqrt(torch.sum(x2*(C @ x2), axis=0) + sigmab ** 2) # shape(n2,)\n",
    "        #print((C @ x2)[19,0])\n",
    "        X1X2 = torch.outer(X1,X2)               #shape(n1,n2)\n",
    "        x1x2 = x1.T @ C @ x2 + sigmab ** 2   \n",
    "        \n",
    "        arg = torch.clip(x1x2 / (X1X2 + 1e-9), -1, 1)\n",
    "\n",
    "        theta = torch.arccos(arg)\n",
    "\n",
    "        J = (torch.sqrt(1 - arg ** 2) + torch.pi * arg - theta * arg) / torch.pi   #shape(n1,n2)\n",
    "\n",
    "        K = X1X2 * J       #shape(n1, n2)\n",
    "        \n",
    "        #print('X1:{},X2:{}, J:{}, X1X2:{}, K:{} '.format(X1.shape, X2.shape, J.shape, X1X2.shape, K.shape))\n",
    "        if dC is not None:\n",
    "            dK = torch.zeros((n1, n2, dC.shape[2] + 1)).to(device)\n",
    "#     dK : array-like\n",
    "#         [n1, n2, ntheta], derivative of kernel with respect to theta\n",
    "\n",
    "            dX1X2 = sigmab ** 2 * (X2 / X1[:, None] + X1[:, None] / X2) #shape same as X1X2\n",
    "\n",
    "            darg = (2 * sigmab ** 2 - arg * dX1X2) / X1X2    #shape same as arg\n",
    "\n",
    "            dJ = -(theta - torch.pi) * darg / torch.pi           #shapesame as J\n",
    "\n",
    "            dK[:, :, 0] =  (X1X2 * dJ + dX1X2 * J)      #shape same as K\n",
    "                                   \n",
    "            for j in range(1, dC.shape[2] + 1):\n",
    "\n",
    "                dX1 = 0.5*torch.sum(x1*(dC[:, :, j-1]@x1), axis=0)/X1  #shape(n1,)\n",
    "                dX2 = 0.5*torch.sum(x2*(dC[:, :, j-1]@x2), axis=0)/X2  #shape(n2,)\n",
    "                \n",
    "                dX1X2 = dX1[:, None]*X2 + X1[:, None]*dX2\n",
    "\n",
    "                darg = (x1.T@(dC[:, :, j-1]@x2) - arg*dX1X2)/X1X2\n",
    "\n",
    "                dJ =  -(theta-torch.pi)*darg/torch.pi\n",
    "\n",
    "                dK[:, :, j] = X1X2*dJ + dX1X2*J\n",
    "            \n",
    "            #print('dX1X2:{}, darg:{}, dJ:{}, dK[:,:,1]:{}'.format(dX1X2.shape,darg.shape, dJ.shape, (A * (X1X2 * dJ + dX1X2 * J)).shape))\n",
    "            # make sure that K is positive definite\n",
    "        if n1==n2:\n",
    "            K = (K+K.T)/2 + 1e-15*torch.eye(n1).to(device)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # return just diagonal of covariance\n",
    "        K = torch.sum(x1*(C@x1), axis=0)[:, None]+sigmab**2\n",
    "\n",
    "\n",
    "        # derivative of covariance\n",
    "        if dC is not None:\n",
    "            dK = torch.zeros((n1,1, dC.shape[2] + 1)).to(device)\n",
    "\n",
    "            dK[:,:, 0] = 2*sigmab**2*torch.ones((n1, 1)).to(device)\n",
    "\n",
    "            for j in range(1, dC.shape[2] + 1):\n",
    "                dK[:,:, j] = torch.sum(x1 * (dC[:, :, j-1]@x1), axis=0)[:, None]\n",
    "\n",
    "            K += 1e-15 * torch.eye(n1, 1).to(device)\n",
    "\n",
    "    if dC is not None: return K.to(torch.float64), dK.to(torch.float64)    #shape(n1,n2), shape(n1,n2,6) \n",
    "    else: return K.to(torch.float64)    #shape (n1,n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 80/80 [12:38<00:00,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# main script\n",
    "\n",
    "#import of the single cell database. 3190 108x108 images and 3190 responses to them\n",
    "data = scipy.io.loadmat(r'/media/samuele/Samuele_02/GP from MAtlab to python/data_cell21.mat')\n",
    "r = data['r']  #shape (1, 3190)\n",
    "X = data['X']  #shape (108,108,3190)\n",
    "\n",
    "Xvec = np.reshape(X, (-1, X.shape[2])) # shape(11664, 3190) each image is considered as a 1D vector\n",
    "\n",
    "# learn hyperparams\n",
    "save_param = 1\n",
    "\n",
    "if save_param==1:\n",
    "    results={}\n",
    "    theta, A, lambda0, m, V, xtilde, L = varGP(Xvec, r, ntilde=250,MaxIter=80, Nmstep=20, Nestep=50, Display=2)\n",
    "    \n",
    "    results['theta']=theta\n",
    "    results['L']=L\n",
    "    results['m']=m\n",
    "    results['V']=V\n",
    "    results['xtilde']=xtilde\n",
    "    results['A']=A\n",
    "    results['lambda0']=lambda0\n",
    "    \n",
    "    np.save('saved_params_cell21_gp-v2.npy', results)\n",
    "    \n",
    "elif save_param ==0:\n",
    "    data = np.load('saved_params_cell21.npy', allow_pickle=True).item()\n",
    "    theta = data['theta']\n",
    "    L = data['L']\n",
    "    m = data['m']\n",
    "    V = data['V']\n",
    "    xtilde = data['xtilde']\n",
    "    A=data['A'][-1]\n",
    "    lambda0=data['lambda0'][-1]\n",
    "    \n",
    "    # compute local sta\n",
    "    X0 = Xvec[:, 31][:, np.newaxis]  #shape(11664,1)\n",
    "\n",
    "    _, idx,_ = localsmoothkern(theta[1:,-1], X0.shape[0])\n",
    "    idx = np.where(idx)[0]\n",
    "\n",
    "    lsta = np.zeros((X0.shape[0],1))\n",
    "\n",
    "    f,sf  = compute_f(m, V, theta[:,-1], A, lambda0, xtilde, X0, kernfun=acoskern)\n",
    "    # print(f,sf)\n",
    "    eta = 1e-5\n",
    "    for i in range(idx.size):         \n",
    "        Xdash = np.copy(X0)\n",
    "        Xdash[idx[i]] = X0[idx[i]]+eta\n",
    "        fdash,_  = compute_f(m, V, theta[:,-1], A, lambda0, xtilde, Xdash, kernfun=acoskern)\n",
    "        lsta[idx[i]] = (fdash-f)/eta\n",
    "\n",
    "    #plot_lsta\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(X0.reshape(108,108))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(lsta[:,0].reshape(108,108))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
